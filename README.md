
# Disfluency removal 
-------------------------
The goal of this project is to transform disfluent questions—those containing hesitations, repetitions, and grammatical inconsistencies—into fluent, coherent, and grammatically correct questions using natural language processing (NLP) models. By leveraging pre-trained transformer-based models such as BART and T5, this project explores data analysis, model training, and evaluation, with a focus on improving model performance through data augmentation and fine-tuning.


| Metric              | Score     |
|---------------------|-----------|
| ROUGE-1             | 0.953951  |
| ROUGE-2             | 0.911474  |
| ROUGE-L             | 0.943273  |
| BLEU                | 0.853771  |
| METEOR              | 0.932146  |
| BERTScore Precision | 0.98963   |
| BERTScore Recall    | 0.987921  |
| BERTScore F1        | 0.988741  |

